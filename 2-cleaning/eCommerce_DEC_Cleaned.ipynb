{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "from pyspark.find_spark_home import _find_spark_home\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(_find_spark_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = os.environ.get('PYTHON_PATH')\n",
    "app_name_dec = os.environ.get('APP_NAME_DEC')\n",
    "hadoop_path_dec = os.environ.get('HADOOP_DEC_DATASET_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.dynamicAllocation.minExecutors', '1')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.driver.memory', '4g')\n",
      "('spark.executor.memory', '4g')\n",
      "('spark.dynamicAllocation.maxExecutors', '10')\n",
      "('spark.driver.host', 'BOOK-G6MUSCB12M.local')\n",
      "('spark.driver.port', '54522')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.id', 'local-1703800372276')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.network.timeout', '800s')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark-local-dir', 'C:\\\\spark-temp')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.driver.maxResultSize', '2g')\n",
      "('spark.pyspark.driver.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '60s')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.app.startTime', '1703800372237')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.app.submitTime', '1703795663467')\n",
      "('spark.app.name', 'ecommerce')\n",
      "('spark.pyspark.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setMaster('local[*]')\\\n",
    "    .set('spark-local-dir', \"C:\\\\spark-temp\")\\\n",
    "    .set('spark.driver.memory', '4g')\\\n",
    "    .set('spark.executor.memory', '4g')\\\n",
    "    .set('spark.driver.maxResultSize', '2g')\\\n",
    "    .set('spark.pyspark.python', python_path)\\\n",
    "    .set('spark.pyspark.driver.python', python_path)\\\n",
    "    .set(\"spark.network.timeout\",\"800s\")\\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", \"10\")\\\n",
    "    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\\\n",
    "\n",
    "spark = SparkSession.builder.appName(app_name_dec).config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for item in sc.getConf().getAll(): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o723.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1545)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530)\r\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:285)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:238)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhadoop_path_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o723.csv.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1545)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530)\r\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:285)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:238)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(hadoop_path_dec, header = True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_index \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mmonotonically_increasing_id())\n\u001b[0;32m      2\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m      4\u001b[0m df_index_first \u001b[38;5;241m=\u001b[39m df_index\u001b[38;5;241m.\u001b[39mselect(column_names)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m \n\u001b[0;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2617\u001b[0m, in \u001b[0;36mDataFrame._jcols\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2616\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2604\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[1;34m(self, cols, converter)\u001b[0m\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[0;32m   2599\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2600\u001b[0m     cols: Sequence,\n\u001b[0;32m   2601\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2602\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[0;32m     70\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:55\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39mfunctions\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\utils.py:202\u001b[0m, in \u001b[0;36mget_active_spark_context\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext or SparkSession should be created first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[1;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "df_index = df.select('*').withColumn('id', F.monotonically_increasing_id())\n",
    "column_names = ['id'] + [col for col in df.columns]\n",
    "\n",
    "df_index_first = df_index.select(column_names)\n",
    "df_index_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+----------+--------------------+-------+-------+---------+\n",
      "| id|         event_time|event_type|product_id|       category_code|  brand|  price|  user_id|\n",
      "+---+-------------------+----------+----------+--------------------+-------+-------+---------+\n",
      "|  0|2019-12-01 01:00:00|      view|   1005105|construction.tool...|  apple|1302.48|556695836|\n",
      "|  1|2019-12-01 01:00:00|      view|  22700068|                null|  force| 102.96|577702456|\n",
      "|  2|2019-12-01 01:00:01|      view|   2402273|appliances.person...|  bosch| 313.52|539453785|\n",
      "|  3|2019-12-01 01:00:02|  purchase|  26400248|computers.periphe...|   null| 132.31|535135317|\n",
      "|  4|2019-12-01 01:00:02|      view|  20100164|    apparel.trousers|   nika| 101.68|517987650|\n",
      "|  5|2019-12-01 01:00:02|      view| 100008256|accessories.umbrella|   ikea| 163.56|542860793|\n",
      "|  6|2019-12-01 01:00:02|      view|  21400264|  electronics.clocks|   null|  88.81|538021416|\n",
      "|  7|2019-12-01 01:00:03|      view|   1005239|construction.tool...| xiaomi| 256.38|525740700|\n",
      "|  8|2019-12-01 01:00:04|      view|   5100885|  computers.notebook|    jet|  20.57|512509221|\n",
      "|  9|2019-12-01 01:00:04|      view|  26205399|construction.comp...|   null| 179.16|553345124|\n",
      "| 10|2019-12-01 01:00:04|      view|  22900009|  computers.notebook|  vegas|  49.94|554369617|\n",
      "| 11|2019-12-01 01:00:04|      view|   1004233|construction.tool...|  apple|1312.52|579969851|\n",
      "| 12|2019-12-01 01:00:05|      view|  22700202|                null|  stels| 171.18|575086722|\n",
      "| 13|2019-12-01 01:00:06|      view|   1004856|construction.tool...|samsung| 124.11|532554953|\n",
      "| 14|2019-12-01 01:00:06|      view|   3701309|appliances.enviro...|polaris|  89.32|543733099|\n",
      "| 15|2019-12-01 01:00:07|      view|  11500445|computers.compone...| xiaomi|  27.77|526844203|\n",
      "| 16|2019-12-01 01:00:07|      view|   1005105|construction.tool...|  apple|1302.48|562071412|\n",
      "| 17|2019-12-01 01:00:07|      view|   1003489|construction.tool...| huawei| 205.67|543826485|\n",
      "| 18|2019-12-01 01:00:07|      view|   1480790|  electronics.clocks| lenovo| 342.82|577653879|\n",
      "| 19|2019-12-01 01:00:07|      view|   4804718|       sport.bicycle|  apple| 329.14|579969767|\n",
      "+---+-------------------+----------+----------+--------------------+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_index_first.drop('user_session', 'category_id')\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+----------+---------------------------------+-------+-------+---------+------------+-----------------------+\n",
      "|id |event_time         |event_type|product_id|category_code                    |brand  |price  |user_id  |category    |product                |\n",
      "+---+-------------------+----------+----------+---------------------------------+-------+-------+---------+------------+-----------------------+\n",
      "|0  |2019-12-01 01:00:00|view      |1005105   |construction.tools.light         |apple  |1302.48|556695836|construction|tools.light            |\n",
      "|1  |2019-12-01 01:00:00|view      |22700068  |null                             |force  |102.96 |577702456|null        |null                   |\n",
      "|2  |2019-12-01 01:00:01|view      |2402273   |appliances.personal.massager     |bosch  |313.52 |539453785|appliances  |personal.massager      |\n",
      "|3  |2019-12-01 01:00:02|purchase  |26400248  |computers.peripherals.printer    |null   |132.31 |535135317|computers   |peripherals.printer    |\n",
      "|4  |2019-12-01 01:00:02|view      |20100164  |apparel.trousers                 |nika   |101.68 |517987650|apparel     |trousers               |\n",
      "|5  |2019-12-01 01:00:02|view      |100008256 |accessories.umbrella             |ikea   |163.56 |542860793|accessories |umbrella               |\n",
      "|6  |2019-12-01 01:00:02|view      |21400264  |electronics.clocks               |null   |88.81  |538021416|electronics |clocks                 |\n",
      "|7  |2019-12-01 01:00:03|view      |1005239   |construction.tools.light         |xiaomi |256.38 |525740700|construction|tools.light            |\n",
      "|8  |2019-12-01 01:00:04|view      |5100885   |computers.notebook               |jet    |20.57  |512509221|computers   |notebook               |\n",
      "|9  |2019-12-01 01:00:04|view      |26205399  |construction.components.faucet   |null   |179.16 |553345124|construction|components.faucet      |\n",
      "|10 |2019-12-01 01:00:04|view      |22900009  |computers.notebook               |vegas  |49.94  |554369617|computers   |notebook               |\n",
      "|11 |2019-12-01 01:00:04|view      |1004233   |construction.tools.light         |apple  |1312.52|579969851|construction|tools.light            |\n",
      "|12 |2019-12-01 01:00:05|view      |22700202  |null                             |stels  |171.18 |575086722|null        |null                   |\n",
      "|13 |2019-12-01 01:00:06|view      |1004856   |construction.tools.light         |samsung|124.11 |532554953|construction|tools.light            |\n",
      "|14 |2019-12-01 01:00:06|view      |3701309   |appliances.environment.vacuum    |polaris|89.32  |543733099|appliances  |environment.vacuum     |\n",
      "|15 |2019-12-01 01:00:07|view      |11500445  |computers.components.power_supply|xiaomi |27.77  |526844203|computers   |components.power_supply|\n",
      "|16 |2019-12-01 01:00:07|view      |1005105   |construction.tools.light         |apple  |1302.48|562071412|construction|tools.light            |\n",
      "|17 |2019-12-01 01:00:07|view      |1003489   |construction.tools.light         |huawei |205.67 |543826485|construction|tools.light            |\n",
      "|18 |2019-12-01 01:00:07|view      |1480790   |electronics.clocks               |lenovo |342.82 |577653879|electronics |clocks                 |\n",
      "|19 |2019-12-01 01:00:07|view      |4804718   |sport.bicycle                    |apple  |329.14 |579969767|sport       |bicycle                |\n",
      "+---+-------------------+----------+----------+---------------------------------+-------+-------+---------+------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.withColumn(\"split_code\", split(col(\"category_code\"), \"\\.\"))\n",
    "\n",
    "df_new = df_new.withColumn(\"category\", col(\"split_code\")[0])\n",
    "\n",
    "df_new = df_new.withColumn(\"product\", expr(\"substring(category_code, length(category) + 2)\"))\n",
    "\n",
    "df_new =df_new.drop(\"split_code\")\n",
    "\n",
    "df_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+----------+-------+-------+---------+------------+-----------------------+\n",
      "|id |event_time         |event_type|product_id|brand  |price  |user_id  |category    |product                |\n",
      "+---+-------------------+----------+----------+-------+-------+---------+------------+-----------------------+\n",
      "|0  |2019-12-01 01:00:00|view      |1005105   |apple  |1302.48|556695836|construction|tools.light            |\n",
      "|1  |2019-12-01 01:00:00|view      |22700068  |force  |102.96 |577702456|null        |null                   |\n",
      "|2  |2019-12-01 01:00:01|view      |2402273   |bosch  |313.52 |539453785|appliances  |personal.massager      |\n",
      "|3  |2019-12-01 01:00:02|purchase  |26400248  |null   |132.31 |535135317|computers   |peripherals.printer    |\n",
      "|4  |2019-12-01 01:00:02|view      |20100164  |nika   |101.68 |517987650|apparel     |trousers               |\n",
      "|5  |2019-12-01 01:00:02|view      |100008256 |ikea   |163.56 |542860793|accessories |umbrella               |\n",
      "|6  |2019-12-01 01:00:02|view      |21400264  |null   |88.81  |538021416|electronics |clocks                 |\n",
      "|7  |2019-12-01 01:00:03|view      |1005239   |xiaomi |256.38 |525740700|construction|tools.light            |\n",
      "|8  |2019-12-01 01:00:04|view      |5100885   |jet    |20.57  |512509221|computers   |notebook               |\n",
      "|9  |2019-12-01 01:00:04|view      |26205399  |null   |179.16 |553345124|construction|components.faucet      |\n",
      "|10 |2019-12-01 01:00:04|view      |22900009  |vegas  |49.94  |554369617|computers   |notebook               |\n",
      "|11 |2019-12-01 01:00:04|view      |1004233   |apple  |1312.52|579969851|construction|tools.light            |\n",
      "|12 |2019-12-01 01:00:05|view      |22700202  |stels  |171.18 |575086722|null        |null                   |\n",
      "|13 |2019-12-01 01:00:06|view      |1004856   |samsung|124.11 |532554953|construction|tools.light            |\n",
      "|14 |2019-12-01 01:00:06|view      |3701309   |polaris|89.32  |543733099|appliances  |environment.vacuum     |\n",
      "|15 |2019-12-01 01:00:07|view      |11500445  |xiaomi |27.77  |526844203|computers   |components.power_supply|\n",
      "|16 |2019-12-01 01:00:07|view      |1005105   |apple  |1302.48|562071412|construction|tools.light            |\n",
      "|17 |2019-12-01 01:00:07|view      |1003489   |huawei |205.67 |543826485|construction|tools.light            |\n",
      "|18 |2019-12-01 01:00:07|view      |1480790   |lenovo |342.82 |577653879|electronics |clocks                 |\n",
      "|19 |2019-12-01 01:00:07|view      |4804718   |apple  |329.14 |579969767|sport       |bicycle                |\n",
      "+---+-------------------+----------+----------+-------+-------+---------+------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new =df_new.drop(\"category_code\")\n",
    "df_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+-------+---------+------------+--------------------+----------+---+\n",
      "| id|event_type|product_id|  brand|  price|  user_id|    category|             product|      date|day|\n",
      "+---+----------+----------+-------+-------+---------+------------+--------------------+----------+---+\n",
      "|  0|      view|   1005105|  apple|1302.48|556695836|construction|         tools.light|2019-12-01|  1|\n",
      "|  1|      view|  22700068|  force| 102.96|577702456|        null|                null|2019-12-01|  1|\n",
      "|  2|      view|   2402273|  bosch| 313.52|539453785|  appliances|   personal.massager|2019-12-01|  1|\n",
      "|  3|  purchase|  26400248|   null| 132.31|535135317|   computers| peripherals.printer|2019-12-01|  1|\n",
      "|  4|      view|  20100164|   nika| 101.68|517987650|     apparel|            trousers|2019-12-01|  1|\n",
      "|  5|      view| 100008256|   ikea| 163.56|542860793| accessories|            umbrella|2019-12-01|  1|\n",
      "|  6|      view|  21400264|   null|  88.81|538021416| electronics|              clocks|2019-12-01|  1|\n",
      "|  7|      view|   1005239| xiaomi| 256.38|525740700|construction|         tools.light|2019-12-01|  1|\n",
      "|  8|      view|   5100885|    jet|  20.57|512509221|   computers|            notebook|2019-12-01|  1|\n",
      "|  9|      view|  26205399|   null| 179.16|553345124|construction|   components.faucet|2019-12-01|  1|\n",
      "| 10|      view|  22900009|  vegas|  49.94|554369617|   computers|            notebook|2019-12-01|  1|\n",
      "| 11|      view|   1004233|  apple|1312.52|579969851|construction|         tools.light|2019-12-01|  1|\n",
      "| 12|      view|  22700202|  stels| 171.18|575086722|        null|                null|2019-12-01|  1|\n",
      "| 13|      view|   1004856|samsung| 124.11|532554953|construction|         tools.light|2019-12-01|  1|\n",
      "| 14|      view|   3701309|polaris|  89.32|543733099|  appliances|  environment.vacuum|2019-12-01|  1|\n",
      "| 15|      view|  11500445| xiaomi|  27.77|526844203|   computers|components.power_...|2019-12-01|  1|\n",
      "| 16|      view|   1005105|  apple|1302.48|562071412|construction|         tools.light|2019-12-01|  1|\n",
      "| 17|      view|   1003489| huawei| 205.67|543826485|construction|         tools.light|2019-12-01|  1|\n",
      "| 18|      view|   1480790| lenovo| 342.82|577653879| electronics|              clocks|2019-12-01|  1|\n",
      "| 19|      view|   4804718|  apple| 329.14|579969767|       sport|             bicycle|2019-12-01|  1|\n",
      "+---+----------+----------+-------+-------+---------+------------+--------------------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.withColumn(\"date\", split(col(\"event_time\"), \" \")[0])\n",
    "df_new = df_new.withColumn(\"day\", split(col(\"date\"), \"-\")[2].cast(IntegerType()))\n",
    "df_new = df_new.drop(\"event_time\")\n",
    "\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-------+-------+---------+------------+--------------------+---+\n",
      "| id|event_type|product_id|  brand|  price|  user_id|    category|             product|day|\n",
      "+---+----------+----------+-------+-------+---------+------------+--------------------+---+\n",
      "|  0|      view|   1005105|  apple|1302.48|556695836|construction|         tools.light|  1|\n",
      "|  1|      view|  22700068|  force| 102.96|577702456|        null|                null|  1|\n",
      "|  2|      view|   2402273|  bosch| 313.52|539453785|  appliances|   personal.massager|  1|\n",
      "|  3|  purchase|  26400248|   null| 132.31|535135317|   computers| peripherals.printer|  1|\n",
      "|  4|      view|  20100164|   nika| 101.68|517987650|     apparel|            trousers|  1|\n",
      "|  5|      view| 100008256|   ikea| 163.56|542860793| accessories|            umbrella|  1|\n",
      "|  6|      view|  21400264|   null|  88.81|538021416| electronics|              clocks|  1|\n",
      "|  7|      view|   1005239| xiaomi| 256.38|525740700|construction|         tools.light|  1|\n",
      "|  8|      view|   5100885|    jet|  20.57|512509221|   computers|            notebook|  1|\n",
      "|  9|      view|  26205399|   null| 179.16|553345124|construction|   components.faucet|  1|\n",
      "| 10|      view|  22900009|  vegas|  49.94|554369617|   computers|            notebook|  1|\n",
      "| 11|      view|   1004233|  apple|1312.52|579969851|construction|         tools.light|  1|\n",
      "| 12|      view|  22700202|  stels| 171.18|575086722|        null|                null|  1|\n",
      "| 13|      view|   1004856|samsung| 124.11|532554953|construction|         tools.light|  1|\n",
      "| 14|      view|   3701309|polaris|  89.32|543733099|  appliances|  environment.vacuum|  1|\n",
      "| 15|      view|  11500445| xiaomi|  27.77|526844203|   computers|components.power_...|  1|\n",
      "| 16|      view|   1005105|  apple|1302.48|562071412|construction|         tools.light|  1|\n",
      "| 17|      view|   1003489| huawei| 205.67|543826485|construction|         tools.light|  1|\n",
      "| 18|      view|   1480790| lenovo| 342.82|577653879| electronics|              clocks|  1|\n",
      "| 19|      view|   4804718|  apple| 329.14|579969767|       sport|             bicycle|  1|\n",
      "+---+----------+----------+-------+-------+---------+------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.drop(\"date\")\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------+-----+---------+------------+-----------------+---+\n",
      "|    id|event_type|product_id|       brand|price|  user_id|    category|          product|day|\n",
      "+------+----------+----------+------------+-----+---------+------------+-----------------+---+\n",
      "|776076|      view|   4804539|      ritmix| 0.87|512983831|  appliances|   kitchen.juicer|  1|\n",
      "|473320|      view|  40500036|       inkax| 2.57|541682579|country_yard|       cultivator|  1|\n",
      "|749788|      view|  58100136| organicshop| 2.65|578899590| accessories|              bag|  1|\n",
      "|379554|      view|   9101106|      ritmix| 3.18|571772259|   furniture|    kitchen.table|  1|\n",
      "|828981|      view|  17400135|viviennesabo| 3.19|522430846|construction|   tools.painting|  1|\n",
      "|690311|      view|  24400518|    floresan|  3.2|516591584|        auto|accessories.winch|  1|\n",
      "|929317|      view|  17600785|      lirene| 3.28|549085369|     apparel|            shoes|  1|\n",
      "|582080|      view|  40500095|       moxom| 3.35|538972887|country_yard|       cultivator|  1|\n",
      "|691643|      view|   9100492|      canyon| 3.84|515384896|   furniture|    kitchen.table|  1|\n",
      "|500712|      view|  18001539|     samsung| 3.84|556395557|construction|    tools.welding|  1|\n",
      "|568121|      view|   9101299|       delux| 4.09|526533073|   furniture|    kitchen.table|  1|\n",
      "|345597|      view|  15901903|        ikea| 4.09|512607862|construction|  tools.generator|  1|\n",
      "|401030|      view| 100008310|       remax| 4.61|569605728|country_yard|       cultivator|  1|\n",
      "|642412|      view|   4802021|      xiaomi| 4.61|520598232|       sport|          bicycle|  1|\n",
      "|737751|      view|   4802021|      xiaomi| 4.61|530613638|       sport|          bicycle|  1|\n",
      "|885653|      view|   8902093|     altacto| 4.65|512730678|   computers|          desktop|  1|\n",
      "|131128|      view|  17601862|     garnier| 4.89|545618308|   furniture| living_room.sofa|  1|\n",
      "| 95261|      view|   4801390|        koss| 4.89|512558795|       sport|          bicycle|  1|\n",
      "|623620|      view|  11600094|    defender| 5.08|524365566|  appliances|   kitchen.kettle|  1|\n",
      "|813616|      view|  18001491|       apple| 5.12|514933939|construction|    tools.welding|  1|\n",
      "+------+----------+----------+------------+-----+---------+------------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.dropna()\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values present in:\n",
      "category: 0\n",
      "product: 0\n",
      "date: 0\n",
      "brand: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Null values present in:\")\n",
    "for c in [\"category\",\"product\",\"date\", \"brand\"]:\n",
    "    print(c +':', df_new.where(F.col(c).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 53,612,307\n"
     ]
    }
   ],
   "source": [
    "count = df_new.count()\n",
    "print(f\"Total number of rows: {count:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+--------+-----------------+--------------------+-----------+-----------------+----------+------------------+\n",
      "|summary|                  id|event_type|          product_id|   brand|            price|             user_id|   category|          product|      date|               day|\n",
      "+-------+--------------------+----------+--------------------+--------+-----------------+--------------------+-----------+-----------------+----------+------------------+\n",
      "|  count|            53612307|  53612307|            53612307|53612307|         53612307|            53612307|   53612307|         53612307|  53612307|          53612307|\n",
      "|   mean|2.968796283566086...|      null|1.5950927373301918E7|     NaN|300.6398401599999|  5.48345002697422E8|       null|             null|      null|17.271609986863652|\n",
      "| stddev|1.734894542916755...|      null|2.7910823423054915E7|     NaN|362.0023991418133|2.8581691692769576E7|       null|             null|      null| 8.578129789960554|\n",
      "|    min|                   0|      cart|             1000544|  a-case|             0.77|            30493659|accessories|accessories.alarm|2019-12-01|                 1|\n",
      "|    max|        592706183725|      view|           100064491|   zyxel|          2574.07|           595414563| stationery|           wallet|2020-01-01|                31|\n",
      "+-------+--------------------+----------+--------------------+--------+-----------------+--------------------+-----------+-----------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = hadoop_path_dec + '/selected_Data/' \n",
    "\n",
    "df_new.repartition(8).write.mode('overwrite').option(\"header\", \"true\").csv(save_path)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
