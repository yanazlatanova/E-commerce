{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Predict the likelihood of a user preferring a specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.find_spark_home import _find_spark_home\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "from pyspark.sql.functions import col, count, isnan, avg, max, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(_find_spark_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = os.environ.get('PYTHON_PATH')\n",
    "app_name = os.environ.get('APP_NAME_DEC')\n",
    "hadoop_path_dec = os.environ.get('HADOOP_DEC_DATASET_PATH')\n",
    "hadoop_path_nov = os.environ.get('HADOOP_NOV_DATASET_PATH')\n",
    "hadoop_path = os.environ.get('HADOOP_DATASET_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.dynamicAllocation.minExecutors', '1')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.app.submitTime', '1703851016297')\n",
      "('spark.driver.memory', '4g')\n",
      "('spark.executor.memory', '4g')\n",
      "('spark.dynamicAllocation.maxExecutors', '10')\n",
      "('spark.driver.host', 'BOOK-G6MUSCB12M.local')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.id', 'local-1703851017548')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.network.timeout', '800s')\n",
      "('spark.driver.port', '60814')\n",
      "('spark.app.startTime', '1703851016447')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark-local-dir', 'C:\\\\spark-temp')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.driver.maxResultSize', '2g')\n",
      "('spark.pyspark.driver.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '60s')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.app.name', 'ecommerce')\n",
      "('spark.pyspark.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setMaster('local[*]')\\\n",
    "    .set('spark-local-dir', \"C:\\\\spark-temp\")\\\n",
    "    .set('spark.driver.memory', '4g')\\\n",
    "    .set('spark.executor.memory', '4g')\\\n",
    "    .set('spark.driver.maxResultSize', '2g')\\\n",
    "    .set('spark.pyspark.python', python_path)\\\n",
    "    .set('spark.pyspark.driver.python', python_path)\\\n",
    "    .set(\"spark.network.timeout\",\"800s\")\\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", \"10\")\\\n",
    "    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\\\n",
    "\n",
    "spark = SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for item in sc.getConf().getAll(): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+---------+-----------+--------------------+---+-----+\n",
      "|event_type|product_id|  brand|  price|  user_id|   category|             product|day|month|\n",
      "+----------+----------+-------+-------+---------+-----------+--------------------+---+-----+\n",
      "|      view|   3600661|samsung| 308.86|572466808| appliances|      kitchen.washer| 17|   11|\n",
      "|      view|   1005136|  apple|2007.52|518751528|electronics|          smartphone| 14|   11|\n",
      "|      view|   1401577|  apple| 1441.2|520999069|  computers|             desktop| 23|   11|\n",
      "|      view|   1005159| xiaomi| 200.39|517708505|electronics|          smartphone| 21|   11|\n",
      "|      cart|   1004766|samsung| 251.82|523183378|electronics|          smartphone| 22|   11|\n",
      "|      view| 100003580| optima| 193.03|519133244| appliances|kitchen.refrigera...| 15|   11|\n",
      "|      view|   1801226|  yasin| 115.75|568638630|electronics|            video.tv| 20|   11|\n",
      "|      view|   1004858|samsung|  126.0|516835814|electronics|          smartphone| 15|   11|\n",
      "|      view|   1003864|   oppo| 231.67|519815419|electronics|          smartphone|  5|   11|\n",
      "|      view|   1005115|  apple| 915.69|519936627|electronics|          smartphone|  6|   11|\n",
      "|      view|   1005077|     bq| 118.15|573927719|electronics|          smartphone| 21|   11|\n",
      "|      view|   4804660| xiaomi|  23.07|548101214|electronics|     audio.headphone| 15|   11|\n",
      "|      view|   1005100|samsung| 136.76|572120627|electronics|          smartphone| 16|   11|\n",
      "|      view|   8800841|  texet|  19.28|516098313|electronics|           telephone| 12|   11|\n",
      "|      view|  13200275|    brw| 525.11|569544291|  furniture|         bedroom.bed| 21|   11|\n",
      "|      view| 100007868| lenovo|1096.63|526642663|  computers|             desktop| 26|   11|\n",
      "|      view|   1005238|   oppo| 308.63|513283781|electronics|          smartphone| 16|   11|\n",
      "|      view|   4804056|  apple| 165.07|568818694|electronics|     audio.headphone| 22|   11|\n",
      "|      view|   1201479| lenovo| 306.06|513994701|electronics|              tablet| 15|   11|\n",
      "|      cart|   4501892|  crown| 104.78|547568205| appliances|         kitchen.hob| 21|   11|\n",
      "+----------+----------+-------+-------+---------+-----------+--------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(hadoop_path + '/combined_Data/', header = True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interaction and purchase counts for categories instead of brands\n",
    "category_interaction_count = df.groupBy(\"user_id\", \"category\").count().withColumnRenamed(\"count\", \"interaction_count\")\n",
    "category_purchase_count = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category\").count().withColumnRenamed(\"count\", \"purchase_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(category_interaction_count, [\"user_id\", \"category\"], \"left\")\n",
    "df = df.join(category_purchase_count, [\"user_id\", \"category\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price interaction and recency for categories\n",
    "avg_price_interaction = df.groupBy(\"user_id\", \"category\").agg(avg(\"price\").alias(\"avg_price_per_category\"))\n",
    "recency_interaction = df.groupBy(\"user_id\", \"category\").agg(max(\"day\").alias(\"last_interaction_day\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join additional calculated features\n",
    "df = df.join(avg_price_interaction, [\"user_id\", \"category\"], \"left\")\n",
    "df = df.join(recency_interaction, [\"user_id\", \"category\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill({\n",
    "    'purchase_count': 0,\n",
    "    'interaction_count': 0,\n",
    "    'avg_price_per_category': 0,\n",
    "    'last_interaction_day': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical 'category' variable\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "df = category_indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex\"], outputCols=[\"categoryVec\"])\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"interaction_count\",\n",
    "        \"purchase_count\",\n",
    "        \"avg_price_per_category\",\n",
    "        \"categoryVec\",\n",
    "        \"last_interaction_day\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable for most preferred category\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"purchase_count\").desc())\n",
    "df = df.withColumn(\"preference_rank\", row_number().over(windowSpec))\n",
    "df = df.withColumn(\"label\", when(col(\"preference_rank\") == 1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, train model, make predictions\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:  0.7658335005889554\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(\"Area Under ROC: \", roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
