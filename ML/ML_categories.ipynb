{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Predict the likelihood of a user preferring a specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.find_spark_home import _find_spark_home\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "from pyspark.sql.functions import col, count, isnan, avg, max, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(_find_spark_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = os.environ.get('PYTHON_PATH')\n",
    "app_name = os.environ.get('APP_NAME_DEC')\n",
    "hadoop_path_dec = os.environ.get('HADOOP_DEC_DATASET_PATH')\n",
    "hadoop_path_nov = os.environ.get('HADOOP_NOV_DATASET_PATH')\n",
    "hadoop_path = os.environ.get('HADOOP_DATASET_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.dynamicAllocation.minExecutors', '1')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.driver.memory', '4g')\n",
      "('spark.executor.memory', '4g')\n",
      "('spark.dynamicAllocation.maxExecutors', '10')\n",
      "('spark.driver.host', 'BOOK-G6MUSCB12M.local')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.startTime', '1703694746431')\n",
      "('spark.app.submitTime', '1703694746188')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.port', '60673')\n",
      "('spark.network.timeout', '800s')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark-local-dir', 'C:\\\\spark-temp')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.driver.maxResultSize', '2g')\n",
      "('spark.pyspark.driver.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '60s')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1703694748087')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.app.name', 'ecommerce')\n",
      "('spark.pyspark.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setMaster('local[*]')\\\n",
    "    .set('spark-local-dir', \"C:\\\\spark-temp\")\\\n",
    "    .set('spark.driver.memory', '4g')\\\n",
    "    .set('spark.executor.memory', '4g')\\\n",
    "    .set('spark.driver.maxResultSize', '2g')\\\n",
    "    .set('spark.pyspark.python', python_path)\\\n",
    "    .set('spark.pyspark.driver.python', python_path)\\\n",
    "    .set(\"spark.network.timeout\",\"800s\")\\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", \"10\")\\\n",
    "    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\\\n",
    "\n",
    "spark = SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for item in sc.getConf().getAll(): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+--------+-------+---------+------------+------------------+---+\n",
      "|          id|event_type|product_id|   brand|  price|  user_id|    category|           product|day|\n",
      "+------------+----------+----------+--------+-------+---------+------------+------------------+---+\n",
      "|257698236188|      view|   3600287| indesit| 205.15|546669693|  appliances|    kitchen.washer| 15|\n",
      "|240518268935|      view|   4803585|  huawei| 115.55|514179359| electronics|   audio.headphone| 15|\n",
      "|549756563931|      view|   4600542|    beko| 357.72|570790007|  appliances|kitchen.dishwasher| 29|\n",
      "|412317698870|      view|  12301593|    zubr|  21.56|516315517|construction|       tools.drill| 19|\n",
      "|257698481187|      view|   1005135|   apple|1644.08|571381066| electronics|        smartphone| 15|\n",
      "| 94489540344|      view|   1002100| samsung| 370.64|560874417| electronics|        smartphone|  7|\n",
      "|      629235|      view|  17000006|      sv| 157.79|534873586|   computers|           desktop|  1|\n",
      "|  8590220575|      view|   5000594| brother| 350.07|536858996|  appliances|    sewing_machine|  1|\n",
      "|180389422564|      view|   2900688|dauscher| 128.68|514988074|  appliances| kitchen.microwave| 13|\n",
      "|429497091119|      view|   1004767| samsung| 246.03|573128067| electronics|        smartphone| 20|\n",
      "|180388691457|      view|   4804056|   apple| 159.57|569718349| electronics|   audio.headphone| 12|\n",
      "|171799174955|      view|   5000184|  janome| 102.11|518743659|  appliances|    sewing_machine| 12|\n",
      "|352187414710|      view|   1005164|  xiaomi| 278.77|512393480| electronics|        smartphone| 17|\n",
      "|558346323511|      view|   1005161|  xiaomi| 192.33|579433200| electronics|        smartphone| 30|\n",
      "|      906586|      view|   1801639|  harper| 179.41|542995228| electronics|          video.tv|  1|\n",
      "|326417685445|      cart|   5100577| samsung| 290.77|514199688| electronics|            clocks| 16|\n",
      "|154619617058|      view|   1801555|      lg| 460.84|534110795| electronics|          video.tv| 11|\n",
      "| 25769983763|      view|   1801854| samsung|1001.05|512932626| electronics|          video.tv|  3|\n",
      "|249108612771|      view|   3700755| samsung|  92.41|560827408|  appliances|environment.vacuum| 15|\n",
      "|335007595086|      view|   3601278|   bosch| 383.02|519171788|  appliances|    kitchen.washer| 17|\n",
      "+------------+----------+----------+--------+-------+---------+------------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(hadoop_path + '/combined_Data/', header = True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interaction and purchase counts for categories instead of brands\n",
    "category_interaction_count = df.groupBy(\"user_id\", \"category\").count().withColumnRenamed(\"count\", \"interaction_count\")\n",
    "category_purchase_count = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category\").count().withColumnRenamed(\"count\", \"purchase_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(category_interaction_count, [\"user_id\", \"category\"], \"left\")\n",
    "df = df.join(category_purchase_count, [\"user_id\", \"category\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price interaction and recency for categories\n",
    "avg_price_interaction = df.groupBy(\"user_id\", \"category\").agg(avg(\"price\").alias(\"avg_price_per_category\"))\n",
    "recency_interaction = df.groupBy(\"user_id\", \"category\").agg(max(\"day\").alias(\"last_interaction_day\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join additional calculated features\n",
    "df = df.join(avg_price_interaction, [\"user_id\", \"category\"], \"left\")\n",
    "df = df.join(recency_interaction, [\"user_id\", \"category\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill({\n",
    "    'purchase_count': 0,\n",
    "    'interaction_count': 0,\n",
    "    'avg_price_per_category': 0,\n",
    "    'last_interaction_day': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical 'category' variable\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "df = category_indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex\"], outputCols=[\"categoryVec\"])\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"interaction_count\",\n",
    "        \"purchase_count\",\n",
    "        \"avg_price_per_category\",\n",
    "        \"categoryVec\",\n",
    "        \"last_interaction_day\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable for most preferred category\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"purchase_count\").desc())\n",
    "df = df.withColumn(\"preference_rank\", row_number().over(windowSpec))\n",
    "df = df.withColumn(\"label\", when(col(\"preference_rank\") == 1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, train model, make predictions\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:  0.7658335005889554\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(\"Area Under ROC: \", roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
