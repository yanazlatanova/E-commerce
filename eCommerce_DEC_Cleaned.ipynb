{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "from pyspark.find_spark_home import _find_spark_home\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(_find_spark_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = os.environ.get('PYTHON_PATH')\n",
    "app_name_dec = os.environ.get('APP_NAME_DEC')\n",
    "hadoop_path_dec = os.environ.get('HADOOP_DEC_DATASET_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.driver.host', '194.47.46.114')\n",
      "('spark.dynamicAllocation.minExecutors', '1')\n",
      "('spark.app.id', 'local-1702898982487')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.dynamicAllocation.maxExecutors', '10')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.maxResultSize', '1g')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.network.timeout', '800s')\n",
      "('spark.app.submitTime', '1702897664999')\n",
      "('spark.driver.port', '57382')\n",
      "('spark.driver.memory', '7g')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark-local-dir', 'C:\\\\spark-temp')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.pyspark.driver.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '60s')\n",
      "('spark.executor.memory', '7g')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.app.startTime', '1702898982389')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.app.name', 'ecommerce')\n",
      "('spark.pyspark.python', 'C:\\\\\\\\Users\\\\\\\\anama\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\Bigdata\\\\\\\\python.exe')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\\\n",
    "    .setMaster('local[*]')\\\n",
    "    .set('spark-local-dir', \"C:\\\\spark-temp\")\\\n",
    "    .set('spark.driver.memory', '7g')\\\n",
    "    .set('spark.executor.memory', '7g')\\\n",
    "    .set('spark.driver.maxResultSize', '1g')\\\n",
    "    .set('spark.pyspark.python', python_path)\\\n",
    "    .set('spark.pyspark.driver.python', python_path)\\\n",
    "    .set(\"spark.network.timeout\",\"800s\")\\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .set(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", \"10\")\\\n",
    "    .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\\\n",
    "\n",
    "spark = SparkSession.builder.appName(app_name_dec).config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for item in sc.getConf().getAll(): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(hadoop_path_dec, header = True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_index \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mmonotonically_increasing_id())\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m \n\u001b[0;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2617\u001b[0m, in \u001b[0;36mDataFrame._jcols\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2616\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2604\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[1;34m(self, cols, converter)\u001b[0m\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[0;32m   2599\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2600\u001b[0m     cols: Sequence,\n\u001b[0;32m   2601\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2602\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[0;32m     70\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\column.py:55\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\u001b[38;5;241m.\u001b[39mfunctions\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[1;32mc:\\Users\\anama\\anaconda3\\envs\\Bigdata\\Lib\\site-packages\\pyspark\\sql\\utils.py:202\u001b[0m, in \u001b[0;36mget_active_spark_context\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext or SparkSession should be created first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[1;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "df_index = df.select('*').withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+-------+-------+---------+\n",
      "|         event_time|event_type|       category_code|  brand|  price|  user_id|\n",
      "+-------------------+----------+--------------------+-------+-------+---------+\n",
      "|2019-12-01 01:00:00|      view|construction.tool...|  apple|1302.48|556695836|\n",
      "|2019-12-01 01:00:00|      view|                null|  force| 102.96|577702456|\n",
      "|2019-12-01 01:00:01|      view|appliances.person...|  bosch| 313.52|539453785|\n",
      "|2019-12-01 01:00:02|  purchase|computers.periphe...|   null| 132.31|535135317|\n",
      "|2019-12-01 01:00:02|      view|    apparel.trousers|   nika| 101.68|517987650|\n",
      "|2019-12-01 01:00:02|      view|accessories.umbrella|   ikea| 163.56|542860793|\n",
      "|2019-12-01 01:00:02|      view|  electronics.clocks|   null|  88.81|538021416|\n",
      "|2019-12-01 01:00:03|      view|construction.tool...| xiaomi| 256.38|525740700|\n",
      "|2019-12-01 01:00:04|      view|  computers.notebook|    jet|  20.57|512509221|\n",
      "|2019-12-01 01:00:04|      view|construction.comp...|   null| 179.16|553345124|\n",
      "|2019-12-01 01:00:04|      view|  computers.notebook|  vegas|  49.94|554369617|\n",
      "|2019-12-01 01:00:04|      view|construction.tool...|  apple|1312.52|579969851|\n",
      "|2019-12-01 01:00:05|      view|                null|  stels| 171.18|575086722|\n",
      "|2019-12-01 01:00:06|      view|construction.tool...|samsung| 124.11|532554953|\n",
      "|2019-12-01 01:00:06|      view|appliances.enviro...|polaris|  89.32|543733099|\n",
      "|2019-12-01 01:00:07|      view|computers.compone...| xiaomi|  27.77|526844203|\n",
      "|2019-12-01 01:00:07|      view|construction.tool...|  apple|1302.48|562071412|\n",
      "|2019-12-01 01:00:07|      view|construction.tool...| huawei| 205.67|543826485|\n",
      "|2019-12-01 01:00:07|      view|  electronics.clocks| lenovo| 342.82|577653879|\n",
      "|2019-12-01 01:00:07|      view|       sport.bicycle|  apple| 329.14|579969767|\n",
      "+-------------------+----------+--------------------+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df.drop('user_session', 'product_id','category_id')\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+---------+-------+---------+\n",
      "|         event_time|event_type|       category_code|    brand|  price|  user_id|\n",
      "+-------------------+----------+--------------------+---------+-------+---------+\n",
      "|2019-12-01 01:00:00|      view|construction.tool...|    apple|1302.48|556695836|\n",
      "|2019-12-01 01:00:01|      view|appliances.person...|    bosch| 313.52|539453785|\n",
      "|2019-12-01 01:00:02|      view|    apparel.trousers|     nika| 101.68|517987650|\n",
      "|2019-12-01 01:00:02|      view|accessories.umbrella|     ikea| 163.56|542860793|\n",
      "|2019-12-01 01:00:03|      view|construction.tool...|   xiaomi| 256.38|525740700|\n",
      "|2019-12-01 01:00:04|      view|  computers.notebook|      jet|  20.57|512509221|\n",
      "|2019-12-01 01:00:04|      view|  computers.notebook|    vegas|  49.94|554369617|\n",
      "|2019-12-01 01:00:04|      view|construction.tool...|    apple|1312.52|579969851|\n",
      "|2019-12-01 01:00:06|      view|construction.tool...|  samsung| 124.11|532554953|\n",
      "|2019-12-01 01:00:06|      view|appliances.enviro...|  polaris|  89.32|543733099|\n",
      "|2019-12-01 01:00:07|      view|computers.compone...|   xiaomi|  27.77|526844203|\n",
      "|2019-12-01 01:00:07|      view|construction.tool...|    apple|1302.48|562071412|\n",
      "|2019-12-01 01:00:07|      view|construction.tool...|   huawei| 205.67|543826485|\n",
      "|2019-12-01 01:00:07|      view|  electronics.clocks|   lenovo| 342.82|577653879|\n",
      "|2019-12-01 01:00:07|      view|       sport.bicycle|    apple| 329.14|579969767|\n",
      "|2019-12-01 01:00:07|      view|construction.tool...|    apple|1312.52|579969851|\n",
      "|2019-12-01 01:00:08|      view|construction.tool...|    apple|  912.5|553704027|\n",
      "|2019-12-01 01:00:08|      view|     apparel.costume| bourjois|   8.25|513133286|\n",
      "|2019-12-01 01:00:08|      view|computers.periphe...|  lucente| 406.45|579425245|\n",
      "|2019-12-01 01:00:09|      view|construction.tool...|prestigio|  66.39|579968273|\n",
      "+-------------------+----------+--------------------+---------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.dropna()\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values present in:\n",
      "category_code: 0\n",
      "brand: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Null values present in:\")\n",
    "for c in [\"category_code\", \"brand\"]:\n",
    "    print(c +':', df_new.where(F.col(c).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 53,612,307\n"
     ]
    }
   ],
   "source": [
    "count = df_new.count()\n",
    "print(f\"Total number of rows: {count:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = hadoop_path_dec + '/selected_Data/' \n",
    "\n",
    "df_new.repartition(8).write.mode('overwrite').csv(save_path)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
